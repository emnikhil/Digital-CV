import streamlit as st

def show_experience():
    st.subheader("Experience & Qualifications")
    st.write(
        """
    - **3.5+ years** of progressive experience in architecting and implementing scalable data pipelines, data warehouse solutions, and cloud-native analytics infrastructure using **Google Cloud Platform (GCP)** services such as **BigQuery**, **Cloud Storage**, **Cloud Composer (Airflow)**, and **Dataproc (Spark)**
    - Extensive hands-on expertise in building and automating end-to-end **ETL/ELT pipelines** using **Python**, **SQL**, and workflow orchestration tools like **Apache Airflow**, enabling real-time and batch data processing for critical business insights
    - Strong background in **cloud data migration**, **data integration**, and **performance optimization**, including optimizing query execution, resource utilization, and pipeline efficiency across large-scale distributed data systems
    - Proven experience in developing **data models**, implementing **data quality checks**, and deploying **secure, fault-tolerant data workflows**, contributing directly to analytics platforms, dashboards, and machine learning applications
    - Collaborated with cross-functional teams to deliver data-driven solutions for clients in industries such as **telecommunications**, **finance**, and **retail**, aligning technical implementation with business needs and KPIs
    - Actively exploring **data science** and **machine learning** domains, with hands-on experience in **Spark MLlib**, data preprocessing, and applied analytics, aiming to bridge engineering with intelligent decision-making
    - **Bachelor of Technology (Honours)** in **Computer Science and Engineering**, with a strong academic foundation in algorithms, distributed systems, databases, and artificial intelligence.
    """
    )
    st.markdown("---")